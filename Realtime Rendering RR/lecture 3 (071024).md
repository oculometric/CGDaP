# projection
model files contain vertices in local space, relative to their origin. thus if we want to transform a model, we need to apply that transform to all their vertices.

projection is the process of converting 3D models and worlds into a 2D image, and we do it with, you guessed it, matrices.

model space -- world matrix --> world space
world space -- view matrix --> view space
view space -- projection matrix --> projection space (homogeneous clip coordinates)
projection space -- screen --> screen space

conceptually, the camera is always at the origin, untransformed (**facing down +Z**). to move the camera, we can just apply the inverse of the camera's transform to transform the whole world (placing the camera back at the origin and maintaining the position of everything else relative to it). *the view matrix is the inverse of the camera's world matrix.*

the projection matrix compresses a frustrum (called the **canonical view volume**) onto a flat plane. **in D3D, Z ranges from 0 to 1.** vertices outside the volume are not rendered.

the two major ways of viewing the scene are perspective (where the far plane of the frustrum is larger than the near, and parallel lines appear to converge) and orthographic (where the frustrum is an oblong and its near and far planes are the same area and are parallel).

the projection matrix adjusts the $X$ coord based on FOV, adjusts $Y$ based on FOV and aspect ratio, and remaps $Z$ to 0-1. conceptually we end up with our vertices mapped into a cube (we squish the frustrum into a cube). we end up in **homogeneous clip space** (homogeneous means we have a $W$ component). in clip space, the cube size is $-W$-$W$ ($W$ is nearly always defined as 1).

in perspective projection, we apply the old depth value as $W$ (to keep it as a note). then to actually do the perspective foreshortening effect, we divide our vector by $W$. the **perspective divide**. now we have **normalized device coordinates** (NDCs), where X and Y are mapped between $-1$ and $1$. the depth value is mapped between $0$ and $1$ but is **NOT** linear. because we're dividing, we get the $\frac{1}{x}$ curve. it is proportional, with most values mapped within the first half.

now these coordinates can easily be converted to pixel coordinates, evaluating the fragment shaders and depth buffering.

the painters algorithm is to just paint far away objects/triangles, and then work closer and closer to the camera. computationally expensive. requires pre-sorting. overlapping objects break. hence, depth buffering is used.

**depth buffering** just means having an image of floats (24 bits for depth, 8 for stencil) where black is 0 (near clip) and white is 1 (far clip).

# the pipeline
input assembler - takes indices and vertices, tells the GPU how to assemble them into geometry. supports different primitives (triangle, line, point). can cause bottlenecks if you're constantly swapping objects in lots of little buffers. **builtin.**

vertex shader - responsible for transforming vertices into clip space. may perform other operations (deformation, skinning, vertex lighting). **required.**

hull shader, tesselator, domain shader - optional stages which can generate new points procedurally. **optional.**

geometry shader - generates new triangles using points from the previous stage. useful for particle effects. **optional.**

rasteriser - not programmable. takes NDCs and calculates which pixels will be affected by a given triangle. interpolates per-vertex values (varyings, normal, UVs, colours, etc) across the triangle. performs clipping, only invokes the pixel shader for visible pixels. can do wireframes too. **builtin.**

pixel shader - returns a colour to be assigned to a pixel. does per-pixel lighting effects. can be used for post-processing. powerful techniques possible via data passed from vertex shader/rasteriser. **required.**

output merger - fixed function. curates the depth buffer and uses it to tell whether or not to write a pixel, updates the stencil buffer, applies blending (transparency is spooky). **builtin.**

compute shaders - not part of the rendering pipeline, but gives you access to the massive parallelisability of GPUs for buffered data to perform raw compute (encryption, data analysis).

**READ THOSE BOOKS**