the CPU does computation. it contains:
- the ALU (arithmetic logic unit) which actually does the maths
- registers which hold data to be operated on, and other important information (current instruction, memory access, instruction pointer)
- cache memory which acts as an intermediate acceleration structure between main memory and registers
- the control unit which instructs the ALU what to do

the CPU has access to various buses. aside from IO buses and PCIe, there are the:
- address bus: specifies a memory location which is being addressed (to be read or written to, also used by IO devices)
- data bus: transfers data between the CPU and memory (or IO devices)
- control bus: responsible for control communication between components, (memory, CPU, IO devices, etc)
the width of the bus determines the size of the data that can be transferred. i.e. a 64-bit address bus can address $2^{64}$ memory location.

fetch-decode-execute cycle:
- fetch the instruction located at IP from memory, place it into CIR
- increment IP
- decode the instruction (into opcode and operands, potentially read more memory)
- execute the instruction (do whatever the opcode says to do)
- repeat

modern CPUs have more than one core, allowing them to execute multiple instructions (or sequences of instructions, aka programs) truly simultaneously.

cache memory vastly accelerates execution. even though memory is fast, it's still glacial compared to the speed the CPU can execute at. cache memory is faster, mostly because it's physically closer to the CPU core. the cache contains pieces of memory that were recently accessed or which the branch predictor thinks will be needed soon, to save waiting for memory to return those values when we actually get there.

cache memory is split into three levels. L1 is specific to each core, and is very small. every physical core has its own L1 cache. L2 cache is larger and shared usually between pairs of physical cores. L3 is much larger again and is shared between the entire CPU.

usually each physical core can support multiple (often 2) logical cores, i.e., they can execute 2 threads truly simultaneously. so for a 6 physical core system, you likely will have 12 logical cores, and thus 12 threads of execution which can run truly concurrently at any one time. that isn't a lot, so the operating system will need to handle swapping threads on and off the CPU to share time using a scheduler.

this time sharing involves a task switch each time a thread gets swapped out for another, which introduces overhead. hence adding more threads to a program eventually slows down, as the CPU spends more and more time just trying performing task switches. thus the ideal setup is to use as many threads as you have logical cores (or a few less), as any more will have to performance benefit and will only add more overhead. there are also other reasons to limit your multithreading! resource sharing etc.

threadpools can be used to complete many tasks in parallel. you keep a queue of tasks, and a pool of threads which check if there are tasks in the queue, and if there are, they remove them and execute them. rinse and repeat. those worker threads may also keep their own local queues which their current task can add new tasks to. however, idle worker threads may also steal work from another worker thread's local queue. local queues are usually FILO in the hope of reducing fetches due to context variables still being in cache; however when queue stealing occurs the stealing thread will steal the oldest task (FIFO) since it's variables will need to be re-fetched regardless.



