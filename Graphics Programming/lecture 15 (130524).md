i dont even know what lecture this is now.

watch firefly apparently

in raytracing land, everything is shiny.

Phong shading -> local illumination
Ray tracing -> global illumination

raytracing produces much more realistic results because it traces actual light rays allowing objects to affect one another ambiently through multiple cascading bounces of indirect light (instead of simple direct light via phong/blinn-phong/gouraud shading).

the downside to raytracing is that it's much, much slower, involving much more maths compared to local illumination methods like blinn-phong.

local illumination models shadows using shadow maps and masks, and emulates reflections using reflection maps.

global illumination gives you back effects like reflection, reflection, true shadows, colour bleeding, and if you use spectral rendering, diffraction!

in the real world: individual photons leave a light source, bounce off object surfaces (where they perform some interaction, altering their colour by absorbing some wavelengths and reflecting others), followed by as many other bounces as happens, before entering the observing device (an eye, camera, etc). a blue object absorbs most wavelengths except blue, instead reflecting blue.

a ray is defined by an **origin** and a **direction**. rays can then intersect with various different types of geometry, intersections which can be computed numerically.

instead of collecting light into the camera, we actually render the scene backwards by shooting rays out of the camera (i.e. we only render what we know we will see).

imagine the rendered image being projected onto the near clipping plane. from each pixel, we shoot a ray out from the camera origin, through the pixel in world space (i.e. we use the camera origin and the pixel position in the world to form a ray). 

that ray is then tested against all the geometry in the world, to determine what it hit, and which was the closest hit. if there was no hit, we set it to be a background colour; if there was a hit, we sample the texture, normal, etc, and modify the colour being carried by the ray based on the surface, and then we reflect that light onwards based on the material's properties.

ray tracing samples lights by tracing rays toward lights to test if they are occluded from the point of view of the origin of the ray.

in simplified terms the colour for a pixel is equal to the colour of the closest surface multiplied by the sum of the colour of visible lights and the colour contributed by subsequent bounces (each of which is also calculated by the same formula).

the equation for reflecting a vector in a normal vector:
$$r = d-2(d\cdot n)n$$

// investigate radiosity rendering!

raytracing has its limitations, particularly depending how light is handled. diffuse surfaces are produced by micro-faceted surfaces with bumpy textures (i.e. they still reflect light, but not in a uniform direction like a mirror does). metallic materials also have odd behaviours in terms of light wavelengths.

raytracing can also do refraction through transparent or translucent materials (transmission). refracted rays are generated when colliding with a surface. snell's law describes how the direction of the light ray changes when hitting water:

$$\frac{i \cdot n}{ r \cdot n} = c$$

ray tracing in directX:

ray generation shaders - generate rays and define how they interact with the scene
hit shaders - executed when a ray hits geometry (closest hit, any hit, intersection), handle shading and material effects
miss shaders - define action rays which don't intersect any geometry
callable shaders - optional shaders which can be invoked by other shaders, allowing for more complex shading routines or procedural stuff

directX automatically does acceleration structures for you, like Bounding Volume Heirarchies, etc.