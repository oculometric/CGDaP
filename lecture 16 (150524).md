shaders!

GL 1 uses a fixed-function pipeline which is very limited. however, modern graphics use programmable pipelines, which means *shaders*.

GL uses GLSL as it's shading language (as does Vulkan), while HLSL is used by DirectX and others. CG is Nvidia's own shading language.

shaders allow you to completely change the rendering pipeline. you can perform your own transformations on vertices, create complex procedural or texture-based effects, and lots and lots more: bump/normal mapping, realtime dynamic shadows, geometry tessellation, and post processing.

this means that graphics happens in two places:
- the CPU sets up models, their vertices, triangles and transformations, and textures and passes them to the GPU
- the GPU then executes the relevant shaders (or in a fixed function pipeline, the built-in 'shaders') producing several screen buffers (depth, normal, colour, and more)

shaders are typically split into two stages:
- the vertex shader - happens once per vertex
- the pixel shader - happens once per pixel (aka fragment shaders)

input assembler -> vertex shader -> hull shader -> tessellator -> domain shader -> geometry shader -> rasteriser -> pixel shader -> output merger -> render targets

vertices in the vertex buffer are completely unmodified (i.e. they're in model coordinates). the vertex shader performs the transformation pipeline, performing the model-to-world-to-view-to-NDC transform, on every single vertex.

the vertex shader might also perform some deformation/displacement per vertex, skinning/animation, or per-vertex lighting. the vertex shader can capture and pass data forwards into subsequent shaders.

tessellation covers three optional pipeline stages:
- hull shader
- tessellator
- domain shader
it's used to compute high detail geometry across lower detail primitives. must be explicitly enabled. might increase or decrease geometry density.

the geometry shader generates additional data from the tessellation stages. for instance, constructing new geometry, per-point sprite generation.

the rasteriser is what converts the vector information (triangles, lines, etc) into a raster image (discrete pixels). interpolates per-vertex values across the triangle like colour, texture coordinates, and normals. defines whether shapes are rendered as wireframes.
[https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm]

the pixel shader is executed for each visible pixel to produce the final image. can perform per-pixel lighting effects (any per-pixel effects in fact), receives various information from previous stages, interpolated normal vectors, interpolated texture coordinates, and so on.

the output merger combines data from previous stages. curates the depth buffer, evaluates depth test rules for stencils and environmental effects, blends, alpha tests, transparentifies, adds, subtracts, and writes to whichever output buffer is specified.

CLIP - non answer. clip coordinates are view-space coordinates before perspective division (divide $x$, $y$, $z$ by $w$) is performed.

compute shaders are non-graphics shaders which just take advantage of the GPU's excellent parallelisation and vector calculations. often involved in encryption.

post processing can use all sorts of data, potentially reading other buffers, producing effects like fog, bloom, motion blur, depth of field, colour correction, and so, so many more.